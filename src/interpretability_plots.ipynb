{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TMJf75LGh14"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as F\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import re\n",
        "import torch.nn as nn\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, models\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import nltk\n",
        "import random\n",
        "from nltk.corpus import stopwords\n",
        "from torchvision import transforms\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    classification_report, multilabel_confusion_matrix,ConfusionMatrixDisplay\n",
        ")\n",
        "from tqdm import tqdm\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import os\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
        "from transformers import AutoModel\n",
        "from torchvision.models import swin_b, Swin_B_Weights\n",
        "import torch.nn.functional as F\n",
        "from typing import Optional, Tuple, Dict\n",
        "from data_preprocessing import MIMIC_MultiModalDataset\n",
        "from mid_fusion import BioFuse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir=\"/data/mimic-cxr/mimic-cxr-jpg\"\n",
        "report_dir=\"/data/reports/\""
      ],
      "metadata": {
        "id": "sckVKNCqG5vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "U-AhU4y1G54c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = MIMIC_MultiModalDataset(\n",
        "    image_dir=image_dir,\n",
        "    report_dir=report_dir,\n",
        "    mode='test',transform=transform\n",
        "    )"
      ],
      "metadata": {
        "id": "D0NI-TPwG6Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "text_encoder_type = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(text_encoder_type)"
      ],
      "metadata": {
        "id": "IwXqrRm2Go6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BioFuse().to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "3Zfn2kNzGpCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_columns = test_dataset.data.drop(\n",
        "    columns=[\"id\", \"findings\", \"paths\", \"Unnamed: 0\", \"file_path\", \"impression\", \"subject_id\", \"study_id\"]\n",
        ").columns.tolist()\n",
        "\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "JGfVjerWGpIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concept_names = [\n",
        "    \"alveolar_opacity\",\n",
        "    \"interstitial_opacity\",\n",
        "    \"focal_lung_opacity\",\n",
        "    \"diffuse_lung_opacity\",\n",
        "    \"blunted_costophrenic_angle\",\n",
        "    \"air_fluid_level\",\n",
        "    \"hyperlucency\",\n",
        "    \"volume_loss\",\n",
        "    \"enlarged_cardiac_silhouette\",\n",
        "    \"mediastinal_widening\",\n",
        "    \"fracture_line\",\n",
        "    \"tube_or_line_present\",\n",
        "    \"lung_mass_or_nodule\",\n",
        "    \"infection_language\",\n",
        "    \"fluid_language\",\n",
        "    \"collapse_language\",\n",
        "    \"device_language\",\n",
        "    \"acute_finding_language\",\n",
        "    \"chronic_finding_language\"\n",
        "]\n",
        "\n",
        "NUM_CONCEPTS = len(concept_names)"
      ],
      "metadata": {
        "id": "1ZEWhRfEGpPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_concepts(report: str):\n",
        "    r = report.lower()\n",
        "    return torch.tensor([\n",
        "        int(any(w in r for w in [\"consolidation\", \"airspace\"])),\n",
        "        int(\"interstitial\" in r),\n",
        "        int(\"focal\" in r),\n",
        "        int(any(w in r for w in [\"diffuse\", \"bilateral\"])),\n",
        "        int(\"costophrenic\" in r),\n",
        "        int(\"air-fluid\" in r),\n",
        "        int(any(w in r for w in [\"hyperlucent\", \"lucency\"])),\n",
        "        int(any(w in r for w in [\"volume loss\", \"collapse\"])),\n",
        "        int(\"cardiomegaly\" in r),\n",
        "        int(\"mediastinal widening\" in r),\n",
        "        int(\"fracture\" in r),\n",
        "        int(any(w in r for w in [\"tube\", \"line\", \"catheter\"])),\n",
        "        int(any(w in r for w in [\"mass\", \"nodule\"])),\n",
        "        int(any(w in r for w in [\"pneumonia\", \"infection\"])),\n",
        "        int(any(w in r for w in [\"effusion\", \"fluid\"])),\n",
        "        int(any(w in r for w in [\"collapse\", \"atelectasis\"])),\n",
        "        int(any(w in r for w in [\"device\", \"support\"])),\n",
        "        int(\"acute\" in r),\n",
        "        int(\"chronic\" in r),\n",
        "    ])"
      ],
      "metadata": {
        "id": "QW6DqPmkHLHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FrozenBioFuseEncoder(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad = False\n",
        "        self.model.eval()\n",
        "\n",
        "    def forward(self, images, input_ids, attention_mask):\n",
        "        with torch.no_grad():\n",
        "            img_feat = self.model.image_encoder(images)\n",
        "            txt_feat = self.model.text_encoder(input_ids, attention_mask)\n",
        "            fused = self.model.fusion(img_feat, txt_feat)\n",
        "        return fused\n"
      ],
      "metadata": {
        "id": "NiMcP0PMHQvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConceptBottleneck(nn.Module):\n",
        "    def __init__(self, embed_dim=768, num_concepts=NUM_CONCEPTS):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(embed_dim, num_concepts)\n",
        "\n",
        "    def forward(self, z):\n",
        "        return torch.sigmoid(self.fc(z))\n"
      ],
      "metadata": {
        "id": "DatBFG_VHQ20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConceptToDisease(nn.Module):\n",
        "    def __init__(self, num_concepts, num_labels):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(num_concepts, num_labels)\n",
        "\n",
        "    def forward(self, concepts):\n",
        "        return self.fc(concepts)\n"
      ],
      "metadata": {
        "id": "ptbiu04YHTa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConceptDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, base_dataset, tokenizer):\n",
        "        self.ds = base_dataset\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.ds[idx]\n",
        "        #print(item)\n",
        "\n",
        "        text = item[\"text\"]\n",
        "        enc = self.tokenizer(\n",
        "            text, padding=\"max_length\", truncation=True,\n",
        "            max_length=128, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"image\": item[\"image\"],\n",
        "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": item[\"label\"],\n",
        "            \"concepts\": extract_text_concepts(text)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "oF4hUrXkHXZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_concept_bottleneck(encoder, cbm, dataloader, device, epochs=15):\n",
        "\n",
        "    opt = torch.optim.Adam(cbm.parameters(), lr=1e-3)\n",
        "    loss_fn = nn.BCELoss()\n",
        "\n",
        "    encoder.eval()\n",
        "    cbm.train()\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        total = 0\n",
        "        for b in dataloader:\n",
        "            z = encoder(\n",
        "                b[\"image\"].to(device),\n",
        "                b[\"input_ids\"].to(device),\n",
        "                b[\"attention_mask\"].to(device),\n",
        "            )\n",
        "            pred = cbm(z)\n",
        "            loss = loss_fn(pred, b[\"concepts\"].float().to(device))\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total += loss.item()\n",
        "\n",
        "        print(f\"[CBM] Epoch {ep+1} | Loss {total/len(dataloader):.4f}\")\n"
      ],
      "metadata": {
        "id": "Ol04s_KBHa86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_surrogate(encoder, cbm, surrogate, dataloader, device, epochs=10):\n",
        "    opt = torch.optim.Adam(surrogate.parameters(), lr=1e-3)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    encoder.eval()\n",
        "    cbm.eval()\n",
        "    surrogate.train()\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        for b in dataloader:\n",
        "            with torch.no_grad():\n",
        "                z = encoder(\n",
        "                    b[\"image\"].to(device),\n",
        "                    b[\"input_ids\"].to(device),\n",
        "                    b[\"attention_mask\"].to(device),\n",
        "                )\n",
        "                c = cbm(z)\n",
        "\n",
        "            logits = surrogate(c)\n",
        "            loss = loss_fn(logits, b[\"labels\"].float().to(device))\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        print(f\"[SUR] Epoch {ep+1} | Loss {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "id": "VA1_vixCHbDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concept_ds = ConceptDataset(train_dataset, tokenizer)\n",
        "loader = torch.utils.data.DataLoader(concept_ds, batch_size=16, shuffle=True)\n",
        "\n",
        "encoder = FrozenBioFuseEncoder(model).to(device)\n",
        "cbm = ConceptBottleneck().to(device)\n",
        "surrogate = ConceptToDisease(NUM_CONCEPTS,num_labels=14).to(device)\n",
        "\n",
        "train_concept_bottleneck(encoder, cbm, loader, device)\n",
        "train_surrogate(encoder, cbm, surrogate, loader, device)\n"
      ],
      "metadata": {
        "id": "0-GfED-oHj-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader=DataLoader(test_dataset,batch_size=32)"
      ],
      "metadata": {
        "id": "Xh1xFLmiHho0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sample_by_id(dataset, sample_id):\n",
        "    for i in range(len(dataset)):\n",
        "        sample = dataset[i]\n",
        "        if sample[\"id\"] == sample_id:\n",
        "            return sample\n",
        "    raise ValueError(f\"Sample with id {sample_id} not found.\")\n"
      ],
      "metadata": {
        "id": "KFNR7-2IHo8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "concept_ds = ConceptDataset(train_dataset, tokenizer)\n",
        "loader = torch.utils.data.DataLoader(concept_ds, batch_size=16, shuffle=True)\n",
        "encoder1 = FrozenBioFuseEncoder(model).to(device)"
      ],
      "metadata": {
        "id": "9k0UzJc3HtTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n"
      ],
      "metadata": {
        "id": "O5Bo6cPkHt05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def explain_sample(image, text, tokenizer, encoder, cbm, device):\n",
        "    enc = tokenizer([text], padding='max_length', truncation=True, max_length=128, return_tensors='pt').to(device)\n",
        "    z = encoder(\n",
        "        image.to(device),\n",
        "        enc['input_ids'],\n",
        "        enc['attention_mask']\n",
        "    )\n",
        "    c = cbm(z).squeeze(0)\n",
        "    concept_dict = {concept_names[i]: float(c[i]) for i in range(len(concept_names))}\n",
        "\n",
        "    return concept_dict\n"
      ],
      "metadata": {
        "id": "psOvfmJ6HzGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor_to_pil(img_tensor):\n",
        "    if img_tensor.dim() == 4:\n",
        "        img_tensor = img_tensor.squeeze(0)\n",
        "    img_tensor = img_tensor.detach().cpu()\n",
        "    img_np = img_tensor.permute(1,2,0).numpy()\n",
        "    img_np = (img_np * 255).astype(np.uint8)\n",
        "    return Image.fromarray(img_np)\n"
      ],
      "metadata": {
        "id": "ohxyW2-UH11L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_concept_explanation(image_tensor, text, encoder, cbm, tokenizer, device):\n",
        "    enc = tokenizer([text], padding='max_length', truncation=True, max_length=128, return_tensors='pt').to(device)\n",
        "    z = encoder(\n",
        "        image_tensor.to(device),\n",
        "        enc['input_ids'],\n",
        "        enc['attention_mask']\n",
        "    )\n",
        "    c = cbm(z).squeeze(0)\n",
        "    concept_dict = {concept_names[i]: float(c[i]) for i in range(len(concept_names))}\n",
        "\n",
        "    return concept_dict\n"
      ],
      "metadata": {
        "id": "nHeJnSPJH22x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_ids = [\n",
        "    \"sub_11984732_idx_1121\",\n",
        "    \"sub_11270948_idx_1479\",\n",
        "    \"sub_11714071_idx_396\",\n",
        "    \"sub_11667471_idx_1073\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "mRc1IlwFH5Xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "for sample_id in sample_ids:\n",
        "    try:\n",
        "        sample = get_sample_by_id(test_dataset, sample_id)\n",
        "        sample_image = sample[\"image\"]\n",
        "        sample_text  = sample[\"text\"]\n",
        "        pil_image = tensor_to_pil(sample_image)\n",
        "        input_image = preprocess(pil_image).unsqueeze(0).to(device)\n",
        "        sample_concepts = explain_sample(\n",
        "            image=input_image,\n",
        "            text=sample_text,\n",
        "            tokenizer=tokenizer,\n",
        "            encoder=encoder,\n",
        "            cbm=cbm,\n",
        "            device=device\n",
        "        )\n",
        "        concept_dict = get_concept_explanation(\n",
        "            image_tensor=input_image,\n",
        "            text=sample_text,\n",
        "            encoder=encoder,\n",
        "            cbm=cbm,\n",
        "            tokenizer=tokenizer,\n",
        "            device=device\n",
        "        )\n",
        "        top_concepts = sorted(concept_dict.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "        top_concepts_str = \"<br>\".join([f\"{k}: {v:.2f}\" for k, v in top_concepts])\n",
        "        results.append({\n",
        "            \"id\": sample_id,\n",
        "            \"text\": sample_text,\n",
        "            \"concept_dict\":concept_dict,\n",
        "            \"top_concepts\": top_concepts,\n",
        "            \"top_concepts_str\": top_concepts_str\n",
        "        })\n",
        "\n",
        "        print(f\"Processed {sample_id}: {top_concepts_str}\")\n",
        "        print(concept_dict)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {sample_id}: {e}\")\n"
      ],
      "metadata": {
        "id": "Q5pqK7NEH8wj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}